apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: strimzi
  name: kafka-prometheus-rules
  namespace: <namespace>
spec:
  groups:
  - name: kafka-api-slo
    rules:
    - alert: ErrorBudgetBurn_ProduceRequests
      annotations:
        message: 'High error budget burn for Failed Produce Requests (current value: {{ $value }})'
      expr: |
        sum(kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate5m{}) > (14.40 * (1-0.90000))
        and
        sum(kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate1h{}) > (14.40 * (1-0.90000))
      for: 2m
      labels:
        name: FailedProduceRequestsPerSec
        severity: critical
    - alert: ErrorBudgetBurn_ProduceRequests
      annotations:
        message: 'High error budget burn for Failed Produce Requests (current value: {{ $value }})'
      expr: |
        sum(kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate30m{}) > (6.00 * (1-0.90000))
        and
        sum(kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate6h{}) > (6.00 * (1-0.90000))
      for: 15m
      labels:
        name: FailedProduceRequestsPerSec
        severity: critical
    - alert: ErrorBudgetBurn_ProduceRequests
      annotations:
        message: 'High error budget burn for Failed Produce Requests (current value: {{ $value }})'
      expr: |
        sum(kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate2h{}) > (3.00 * (1-0.90000))
        and
        sum(kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate1d{}) > (3.00 * (1-0.90000))
      for: 1h
      labels:
        name: FailedProduceRequestsPerSec
        severity: warning
    - alert: ErrorBudgetBurn_ProduceRequests
      annotations:
        message: 'High error budget burn for Failed Produce Requests (current value: {{ $value }})'
      expr: |
        sum(kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate6h{}) > (1.00 * (1-0.90000))
        and
        sum(kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate3d{}) > (1.00 * (1-0.90000))
      for: 3h
      labels:
        name: FailedProduceRequestsPerSec
        severity: warning
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_produce_requests_total{}[1d]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_produce_requests_total{}[1d]))
      labels:
        name: FailedProduceRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate1d
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_produce_requests_total{}[1h]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_produce_requests_total{}[1h]))
      labels:
        name: FailedProduceRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate1h
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_produce_requests_total{}[2h]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_produce_requests_total{}[2h]))
      labels:
        name: FailedProduceRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate2h
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_produce_requests_total{}[30m]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_produce_requests_total{}[30m]))
      labels:
        name: FailedProduceRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate30m
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_produce_requests_total{}[3d]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_produce_requests_total{}[3d]))
      labels:
        name: FailedProduceRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate3d
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_produce_requests_total{}[5m]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_produce_requests_total{}[5m]))
      labels:
        name: FailedProduceRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate5m
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_produce_requests_total{}[6h]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_produce_requests_total{}[6h]))
      labels:
        name: FailedProduceRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_produce_requests_total:burnrate6h
    - alert: ErrorBudgetBurn_FetchRequests
      annotations:
        message: 'High error budget burn for Failed Fetch Requests (current value: {{ $value }})'
      expr: |
        sum(kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate5m{}) > (14.40 * (1-0.90000))
        and
        sum(kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate1h{}) > (14.40 * (1-0.90000))
      for: 2m
      labels:
        name: FailedFetchRequestsPerSec
        severity: critical
    - alert: ErrorBudgetBurn_FetchRequests
      annotations:
        message: 'High error budget burn for Failed Fetch Requests (current value: {{ $value }})'
      expr: |
        sum(kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate30m{}) > (6.00 * (1-0.90000))
        and
        sum(kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate6h{}) > (6.00 * (1-0.90000))
      for: 15m
      labels:
        name: FailedFetchRequestsPerSec
        severity: critical
    - alert: ErrorBudgetBurn_FetchRequests
      annotations:
        message: 'High error budget burn for Failed Fetch Requests (current value: {{ $value }})'
      expr: |
        sum(kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate2h{}) > (3.00 * (1-0.90000))
        and
        sum(kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate1d{}) > (3.00 * (1-0.90000))
      for: 1h
      labels:
        name: FailedFetchRequestsPerSec
        severity: warning
    - alert: ErrorBudgetBurn_FetchRequests
      annotations:
        message: 'High error budget burn for Failed Fetch Requests (current value: {{ $value }})'
      expr: |
        sum(kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate6h{}) > (1.00 * (1-0.90000))
        and
        sum(kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate3d{}) > (1.00 * (1-0.90000))
      for: 3h
      labels:
        name: FailedFetchRequestsPerSec
        severity: warning
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_fetch_requests_total{}[1d]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_fetch_requests_total{}[1d]))
      labels:
        name: FailedFetchRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate1d
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_fetch_requests_total{}[1h]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_fetch_requests_total{}[1h]))
      labels:
        name: FailedFetchRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate1h
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_fetch_requests_total{}[2h]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_fetch_requests_total{}[2h]))
      labels:
        name: FailedFetchRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate2h
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_fetch_requests_total{}[30m]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_fetch_requests_total{}[30m]))
      labels:
        name: FailedFetchRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate30m
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_fetch_requests_total{}[3d]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_fetch_requests_total{}[3d]))
      labels:
        name: FailedFetchRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate3d
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_fetch_requests_total{}[5m]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_fetch_requests_total{}[5m]))
      labels:
        name: FailedFetchRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate5m
    - expr: |
        sum(rate(kafka_server_brokertopicmetrics_failed_fetch_requests_total{}[6h]))
        /
        sum(rate(kafka_server_brokertopicmetrics_total_fetch_requests_total{}[6h]))
      labels:
        name: FailedFetchRequestsPerSec
      record: kafka_server_brokertopicmetrics_failed_fetch_requests_total:burnrate6h
    - alert: ErrorBudgetBurn_Connections
      annotations:
        message: 'High error budget burn for haproxy connection errors (current value: {{ $value }})'
      expr: |
        sum(haproxy_server_connection_errors_total:burnrate5m) > (14.40 * (1-0.90000))
        and
        sum(haproxy_server_connection_errors_total:burnrate1h) > (14.40 * (1-0.90000))
      for: 2m
      labels:
        name: FailedConnectionsPerSec
        severity: critical
    - alert: ErrorBudgetBurn_Connections
      annotations:
        message: 'High error budget burn for haproxy connection errors (current value: {{ $value }})'
      expr: |
        sum(haproxy_server_connection_errors_total:burnrate30m) > (6.00 * (1-0.90000))
        and
        sum(haproxy_server_connection_errors_total:burnrate6h) > (6.00 * (1-0.90000))
      for: 15m
      labels:
        name: FailedConnectionsPerSec
        severity: critical
    - alert: ErrorBudgetBurn_Connections
      annotations:
        message: 'High error budget burn for haproxy connection errors (current value: {{ $value }})'
      expr: |
        sum(haproxy_server_connection_errors_total:burnrate2h) > (3.00 * (1-0.90000))
        and
        sum(haproxy_server_connection_errors_total:burnrate1d) > (3.00 * (1-0.90000))
      for: 1h
      labels:
        name: FailedConnectionsPerSec
        severity: warning
    - alert: ErrorBudgetBurn_Connections
      annotations:
        message: 'High error budget burn for haproxy connection errors (current value: {{ $value }})'
      expr: |
        sum(haproxy_server_connection_errors_total:burnrate6h) > (1.00 * (1-0.90000))
        and
        sum(haproxy_server_connection_errors_total:burnrate3d) > (1.00 * (1-0.90000))
      for: 3h
      labels:
        name: FailedConnectionsPerSec
        severity: warning
    - expr: |
        sum(rate(haproxy_server_connection_errors_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[1d]))
        /
        sum(rate(haproxy_server_connections_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[1d]))
      labels:
        name: FailedConnectionsPerSec
      record: haproxy_server_connection_errors_total:burnrate1d
    - expr: |
        sum(rate(haproxy_server_connection_errors_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[1h]))
        /
        sum(rate(haproxy_server_connections_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[1h]))
      labels:
        name: FailedConnectionsPerSec
      record: haproxy_server_connection_errors_total:burnrate1h
    - expr: |
        sum(rate(haproxy_server_connection_errors_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[2h]))
        /
        sum(rate(haproxy_server_connections_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[2h]))
      labels:
        name: FailedConnectionsPerSec
      record: haproxy_server_connection_errors_total:burnrate2h
    - expr: |
        sum(rate(haproxy_server_connection_errors_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[30m]))
        /
        sum(rate(haproxy_server_connections_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[30m]))
      labels:
        name: FailedConnectionsPerSec
      record: haproxy_server_connection_errors_total:burnrate30m
    - expr: |
        sum(rate(haproxy_server_connection_errors_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[3d]))
        /
        sum(rate(haproxy_server_connections_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[3d]))
      labels:
        name: FailedConnectionsPerSec
      record: haproxy_server_connection_errors_total:burnrate3d
    - expr: |
        sum(rate(haproxy_server_connection_errors_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[5m]))
        /
        sum(rate(haproxy_server_connections_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[5m]))
      labels:
        name: FailedConnectionsPerSec
      record: haproxy_server_connection_errors_total:burnrate5m
    - expr: |
        sum(rate(haproxy_server_connection_errors_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[6h]))
        /
        sum(rate(haproxy_server_connections_total{route=~".+-kafka-([0-9]+|bootstrap)$"}[6h]))
      labels:
        name: FailedConnectionsPerSec
      record: haproxy_server_connection_errors_total:burnrate6h
  - name: kafka
    rules:
    - alert: KafkaPersistentVolumeFillingUp
      expr: kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-([0-9]+)?-(.+)-kafka-[0-9]+"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"data-([0-9]+)?-(.+)-kafka-[0-9]+"} < 0.03
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: 'Kafka Broker PersistentVolume is filling up.'
        description: 'The Kafka Broker PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.'
    - alert: KafkaPersistentVolumeFillingUp
      expr: (kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-([0-9]+)?-(.+)-kafka-[0-9]+"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"data-([0-9]+)?-(.+)-kafka-[0-9]+"} < 0.15) and predict_linear(kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-([0-9]+)?-(.+)-kafka-[0-9]+"}[6h], 4 * 24 * 3600) < 0
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: 'Kafka Broker PersistentVolume is filling up.'
        description: 'Based on recent sampling, the Kafka Broker PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.'
    - alert: UnderReplicatedPartitions
      expr: kafka_server_replicamanager_under_replicated_partitions > 0
      for: 10s
      labels:
        severity: warning
      annotations:
        summary: 'Kafka under replicated partitions'
        description: 'There are {{ $value }} under replicated partitions on {{ $labels.kubernetes_pod_name }}'
    - alert: AbnormalControllerState
      expr: sum(kafka_controller_kafkacontroller_active_controller_count) != 1
      for: 10s
      labels:
        severity: warning
      annotations:
        summary: 'Kafka abnormal controller state'
        description: 'There are {{ $value }} active controllers in the cluster'
    - alert: OfflinePartitions
      expr: sum(kafka_controller_kafkacontroller_offline_partitions_count) > 0
      for: 10s
      labels:
        severity: warning
      annotations:
        summary: 'Kafka offline partitions'
        description: 'One or more partitions have no leader'
    - alert: UnderMinIsrPartitionCount
      expr: kafka_server_replicamanager_under_min_isr_partition_count > 0
      for: 10s
      labels:
        severity: warning
      annotations:
        summary: 'Kafka under min ISR partitions'
        description: 'There are {{ $value }} partitions under the min ISR on {{ $labels.kubernetes_pod_name }}'
    - alert: OfflineLogDirectoryCount
      expr: kafka_log_logmanager_offline_log_directory_count > 0
      for: 10s
      labels:
        severity: warning
      annotations:
        summary: 'Kafka offline log directories'
        description: 'There are {{ $value }} offline log directories on {{ $labels.kubernetes_pod_name }}'
    - alert: ScrapeProblem
      expr: up{kubernetes_namespace!~"openshift-.+",kubernetes_pod_name=~".+-kafka-[0-9]+"} == 0
      for: 3m
      labels:
        severity: major
      annotations:
        summary: 'Prometheus unable to scrape metrics from {{ $labels.kubernetes_pod_name }}/{{ $labels.instance }}'
        description: 'Prometheus was unable to scrape metrics from {{ $labels.kubernetes_pod_name }}/{{ $labels.instance }} for more than 3 minutes'
    - alert: ClusterOperatorContainerDown
      expr: count((container_last_seen{container="strimzi-cluster-operator"} > (time() - 90))) < 1 or absent(container_last_seen{container="strimzi-cluster-operator"})
      for: 1m
      labels:
        severity: major
      annotations:
        summary: 'Cluster Operator down'
        description: 'The Cluster Operator has been down for longer than 90 seconds'
    - alert: KafkaBrokerContainersDown
      expr: absent(container_last_seen{container="kafka",pod=~".+-kafka-[0-9]+"})
      for: 3m
      labels:
        severity: major
      annotations:
        summary: 'All `kafka` containers down or in CrashLookBackOff status'
        description: 'All `kafka` containers have been down or in CrashLookBackOff status for 3 minutes'
    - alert: KafkaContainerRestartedInTheLast5Minutes
      expr: count(count_over_time(container_last_seen{container="kafka"}[5m])) > 2 * count(container_last_seen{container="kafka",pod=~".+-kafka-[0-9]+"})
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: 'One or more Kafka containers restarted too often'
        description: 'One or more Kafka containers were restarted too often within the last 5 minutes'
  - name: zookeeper
    rules:
    - alert: AvgRequestLatency
      expr: zookeeper_avg_request_latency > 10
      for: 10s
      labels:
        severity: warning
      annotations:
        summary: 'Zookeeper average request latency'
        description: 'The average request latency is {{ $value }} on {{ $labels.kubernetes_pod_name }}'
    - alert: OutstandingRequests
      expr: zookeeper_outstanding_requests > 10
      for: 10s
      labels:
        severity: warning
      annotations:
        summary: 'Zookeeper outstanding requests'
        description: 'There are {{ $value }} outstanding requests on {{ $labels.kubernetes_pod_name }}'
    - alert: ZookeeperPersistentVolumeFillingUp
      expr: kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-(.+)-zookeeper-[0-9]+"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"data-(.+)-zookeeper-[0-9]+"} < 0.03
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: 'Zookeeper PersistentVolume is filling up.'
        description: 'The Zookeeper PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.'
    - alert: ZookeeperPersistentVolumeFillingUp
      expr: (kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-(.+)-zookeeper-[0-9]+"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"data-(.+)-zookeeper-[0-9]+"} < 0.15) and predict_linear(kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-(.+)-zookeeper-[0-9]+"}[6h], 4 * 24 * 3600) < 0
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: 'Zookeeper PersistentVolume is filling up.'
        description: 'Based on recent sampling, the Zookeeper PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.'
    - alert: ZookeeperContainerRestartedInTheLast5Minutes
      expr: count(count_over_time(container_last_seen{container="zookeeper"}[5m])) > 2 * count(container_last_seen{container="zookeeper",pod=~".+-zookeeper-[0-9]+"})
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: 'One or more Zookeeper containers were restarted too often'
        description: 'One or more Zookeeper containers were restarted too often within the last 5 minutes. This alert can be ignored when the Zookeeper cluster is scaling up'
    - alert: ZookeeperContainersDown
      expr: absent(container_last_seen{container="zookeeper",pod=~".+-zookeeper-[0-9]+"})
      for: 3m
      labels:
        severity: major
      annotations:
        summary: 'All `zookeeper` containers in the Zookeeper pods down or in CrashLookBackOff status'
        description: 'All `zookeeper` containers in the Zookeeper pods have been down or in CrashLookBackOff status for 3 minutes'
  - name: kafkaExporter
    rules:
    - alert: UnderReplicatedPartition
      expr: kafka_topic_partition_under_replicated_partition > 0
      for: 10s
      labels:
        severity: warning
      annotations:
        summary: 'Topic has under-replicated partitions'
        description: 'Topic  {{ $labels.topic }} has {{ $value }} under-replicated partition {{ $labels.partition }}'
    - alert: TooLargeConsumerGroupLag
      expr: kafka_consumergroup_lag > 1000
      for: 10s
      labels:
        severity: warning
      annotations:
        summary: 'Consumer group lag is too big'
        description: 'Consumer group {{ $labels.consumergroup}} lag is too big ({{ $value }}) on topic {{ $labels.topic }}/partition {{ $labels.partition }}'
    - alert: NoMessageForTooLong
      expr: changes(kafka_topic_partition_current_offset{topic!="__consumer_offsets"}[10m]) == 0
      for: 10s
      labels:
        severity: warning
      annotations:
        summary: 'No message for 10 minutes'
        description: 'There is no messages in topic {{ $labels.topic}}/partition {{ $labels.partition }} for 10 minutes'